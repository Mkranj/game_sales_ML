---
title: "Model showcase"
output: html_document
date: "2023-11-19"
---

Predicting global game sales by video game characteristics.

An interesting model will showcase some underlying characteristics that could be
unexpectedly linked to sale numbers. Of course, true precision, unexpected smash
hits are very difficult to predict. However, this might shed light on some
attributes that normally wouldn't even be considered - e.g. game title
characteristics.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
source("feature_engineering_fn.R")
source("post_process_fn.R")
source("visualisation_fn.R")
```
```{r}
source("1_load.R")
```

# Visualisations

```{r}
library(ggplot2)
ggplot(games, aes(x = Global_Sales)) + geom_histogram()
```

Overview without extremes

```{r}

ggplot(games %>% 
         filter(Global_Sales < 2), 
       aes(x = Global_Sales)) + geom_histogram()
```
```{r}

ggplot(games %>% 
         filter(Global_Sales < 0.5), 
       aes(x = Global_Sales)) + geom_histogram()
```
# Sales and number of consoles released on
```{r}
sales_by_cons_no <- group_by(games, Consoles_no) %>%
  summarise(average = mean(Global_Sales),
            count = n()
  ) %>%
  mutate(average_per_console = average/Consoles_no)

ggplot(sales_by_cons_no, aes(x = Consoles_no, y = average)) +
  geom_point(aes(size = count)) + scale_x_continuous(breaks = 1:20)

ggplot(sales_by_cons_no, aes(x = Consoles_no, y = average_per_console)) + geom_point(aes(size = count)) + scale_x_continuous(breaks = 1:20)
# Not linear - polynomial might helps predictions
```
```{r}
# Draw a smooth line considering all points, and polyinomial of a given level.
# Summary "dots" in the background for easier viewing

ggplot_discrete_polynomials(games, "Consoles_no", "Global_Sales", poly_degree = 1)
ggplot_discrete_polynomials(games, "Consoles_no", "Global_Sales", poly_degree = 2)
ggplot_discrete_polynomials(games, "Consoles_no", "Global_Sales", poly_degree = 3)
```

# Exploring game names
```{r}
game_names <- games$Name
subtitles <- count_subtitles(game_names)

# Making it into variables:
games$has_subtitle <- subtitles$subtitle_present
games$subtitle_count <- subtitles$subtitle_count
```

```{r}
by_subtitle <- group_by(games, has_subtitle) %>%
  summarise(avg = mean(Global_Sales), 
            std = sd(Global_Sales),
            no = n())

no_subtitles <- group_by(games, subtitle_count) %>%
  summarise(avg = mean(Global_Sales), 
            std = sd(Global_Sales),
            no = n())

# Actually BOTH hold!!!
print(by_subtitle)
print(no_subtitles)
```
Detect unusual titles  
Those with non-alphanumeric symbols - count how many there are
```{r}
games$unusual_title_symbols <- games$Name %>% count_unusual_symbols()
```

```{r}
unusual_titles <- group_by(games, unusual_title_symbols) %>%
  summarise(avg = mean(Global_Sales), 
            std = sd(Global_Sales),
            no = n())

unusual_titles 
```

```{r}
ggplot_discrete_polynomials(games, 
                            "unusual_title_symbols", "Global_Sales",
                            poly_degree = 1)
ggplot_discrete_polynomials(games, 
                            "unusual_title_symbols", "Global_Sales",
                            poly_degree = 2)
ggplot_discrete_polynomials(games, 
                            "unusual_title_symbols", "Global_Sales",
                            poly_degree = 3)
```

This does appear to lower sales. One degree should be enough.

Title wordcount
```{r}
ggplot_discrete_polynomials(games, "Title_words", "Global_Sales", 1)
```
Title length
```{r}
ggplot_discrete_polynomials(games, "Title_length", "Global_Sales", 1)
ggplot_discrete_polynomials(games, "Title_length", "Global_Sales", 2)
```
Second level polynomial could be useful

Interaction of wordcount and length
```{r}
ggplot(games, aes(x = Title_length, y = Title_words)) +
  geom_jitter(
    aes(color = Global_Sales),
    # Amount to jitter the points for better overview
    width = 0.2, height = 0.4)

ggplot(games, aes(x = Title_length, y = Title_words)) +
  geom_jitter(
    aes(color = log(Global_Sales))
    # Amount to jitter the points for better overview
    #,width = 0.2, height = 0.4
    )
```
It's hard to see a pattern, interaction probably wouldn't increase prediction accuraccy.

## By console
```{r}
for (console in console_colnames) {
  group_by(games, .data[[console]]) %>% summarise(avg = mean(Global_Sales), 
            std = sd(Global_Sales),
            no = n()) %>% print()
}
```


# Modelling
Criterium: Global sales

Training-testing split
```{r}
tt_split <- initial_split(games)

tt_training <- training(tt_split)
tt_testing <- testing(tt_split)

```

Feature engineering

```{r}
dev_summary <- calculate_dev_summaries(tt_training)
head(dev_summary)
```

```{r}
# Join games' dev avg_income and games made total

# Special object for joining, just adjust variable names to start with dev_
devs_join <- dev_summary %>%
  rename_with( ~ paste0("dev_", .x))

tt_training <- left_join(tt_training, devs_join, by = c("Developer" = "dev_Developer"))
tt_testing <- left_join(tt_testing, devs_join, by = c("Developer" = "dev_Developer"))
```

```{r}
# ADJUSTMENT
# if developers are not recognised, set avg_sales and games made to zero!
# Seems more sensical than using the median to imput - if the dev team is
# small, they should get lower values here

tt_training <- tt_training %>% missing_to_zero("dev_avg_income")
tt_training <- tt_training %>% missing_to_zero("dev_no_games")

tt_testing <- tt_testing %>% missing_to_zero("dev_avg_income")
tt_testing <- tt_testing %>% missing_to_zero("dev_no_games")
```


Visualisation of sales by dev characteristics
```{r}
ggplot(tt_training, aes(x = dev_avg_income, y = Global_Sales)) + geom_point() +
  geom_smooth()


ggplot(tt_training, aes(x = dev_no_games, y = Global_Sales)) + geom_point() +
  geom_smooth()
```
Average income correlates with global sales, of course. Number of games made not so much.

```{r}
# Is the game developed by a dev that's top 10 by income, or by
# amount of games made?

# NOT USED IN MODEL ANYMORE

tt_training$Top_performer_dev <- 
  mark_top_performer_dev(input_df = tt_training,
                         summarised_df = dev_summary,
                         summary_metric = "avg_income",
                         n_top = 10) |> 
  as.factor()

tt_testing$Top_performer_dev <- 
  mark_top_performer_dev(input_df = tt_testing,
                         summarised_df = dev_summary,
                         summary_metric = "avg_income",
                         n_top = 10) |> 
  as.factor()

tt_training$Dev_games_made <- 
  mark_top_performer_dev(input_df = tt_training,
                         summarised_df = dev_summary,
                         summary_metric = "no_games",
                         n_top = 10) |> 
  as.factor()

tt_testing$Dev_games_made <- 
  mark_top_performer_dev(input_df = tt_testing,
                         summarised_df = dev_summary,
                         summary_metric = "no_games",
                         n_top = 10) |> 
  as.factor()
```

Squared number of Consoles released on
```{r}
tt_training$Consoles_no_sq <- tt_training$Consoles_no ** 2
tt_testing$Consoles_no_sq <- tt_testing$Consoles_no ** 2
```


Model definition

```{r}
library(recipes)
# Define variables to be used
predictors <- c("Year_release_n", "Genre_F", "Rating_F", "dev_avg_income",  "subtitle_count",
                console_colnames, "Consoles_no", "unusual_title_symbols",
                "Title_words", "Title_length")
criterium <- "Global_Sales"

predictors_to_center <- c("Year_release_n", "dev_avg_income", "unusual_title_symbols", "Title_words", "Title_length")

model_formula <- paste0(criterium, " ~ ",
                        paste(predictors, collapse = " + ")) |> 
  formula()

model_recipe <- recipe(model_formula, data = tt_training) |> 
  step_impute_median(Year_release_n) |> 
  step_impute_mode(all_nominal()) |> 
  step_dummy(all_factor_predictors()) |> 
  step_poly(Consoles_no, degree = 2) |>
  step_poly(subtitle_count, degree = 2) |>
  step_center(predictors_to_center,
              starts_with(c("Consoles_no", "subtitle_count"))
              ) |> 
  step_scale(predictors_to_center,
             starts_with(c("Consoles_no", "subtitle_count"))
             )

# Postprocessing - can't be negative

linear_model <- linear_reg()
```

Fit model

```{r}
model_workflow <- workflow() |>
  add_model(linear_model) |> 
  add_recipe(model_recipe)

fitted_model <- model_workflow |> fit(tt_training)

# Metrics



training_results <- tt_testing |> 
  #select(Global_Sales) |> 
  bind_cols(
    predict(fitted_model, new_data = tt_testing[, predictors])
  ) |>
  adjust_negative_preds()

# show that all the preprocessing steps are applied to the testing data too
# (at predict() stage)
#prep(model_recipe) |>  bake(new_data = tt_testing)

training_results |> metrics(truth = Global_Sales, estimate = .pred)

```

Testing data with augmented features
```{r}
prep(model_recipe) %>%
  bake(new_data = tt_testing) %>%
  bind_cols(
    predict(fitted_model, new_data = tt_testing[, predictors])
  ) |>
  adjust_negative_preds()
```


Historic:
"Year_release_n", "Genre_F", "Rating_F", "Top_performer_dev", "Dev_games_made"
linear model
rsq 18.9%
mae 0.56

random forest
rsq 29%
mae 0.53